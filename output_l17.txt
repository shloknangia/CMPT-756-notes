The goal of a replicated state machine is to keep asynchronous machines in a system working, even in the presence of faults. Non-malicious faults can include things like communication errors, dropped messages, or system failures. Replicated state machines work by having each machine maintain a copy of the state of the system, and applying a set of commands to transition to the current state. This approach is based on fault tolerance systems that have been implemented in real-world distributed systems.

Raft is a consensus algorithm for distributed systems that is designed to be easy to understand and implement. It uses two RPC calls for communication between servers, and has a strong leader that the other servers follow.

Raft is a consensus algorithm that is used to manage a replicated log. It uses a leader-based system where each node in the cluster can vote for leaders and then that leader can force all of its decisions to the followers. This makes it easier because in practice, everyone can come in and propose values and the leader can accept or reject them. If the leader fails, another leader is elected.

The goal of a majority-based system is to keep all of the nodes in the system in a consistent state. The leader node accepts what is going to make it to the log and forces everyone else to do the same. This ensures that all of the nodes in the system are working the same way and that no commands are applied differently to different nodes.

A leader election is held at the beginning of each term. If a leader is not elected, another election is held. This process continues until a leader is elected.

The election process in Raft involves nodes nominating themselves and other nodes voting for candidates. If a majority of nodes vote for the same candidate, that candidate becomes the leader. If there is no majority, or if multiple candidates receive the same number of votes, the process repeats.

The vote for leader or not process is similar to what was done in Paxos - the most recent one is selected as the leader. This is done by selecting the leader with the higher term number, as they are assumed to have the most recent log. This process is randomized by having different timeouts for leader election, which ensures that eventually a leader with a higher term number will be elected.

The leader election process in a distributed system is as follows: at startup, all nodes are followers and one node is elected as leader. If the leader fails or a new node joins the system, a new leader is elected by the followers. The leader maintains its leadership by sending heartbeat messages to the followers. If a follower does not receive heartbeat messages from the leader, it assumes that the leader has failed and starts its own election.

Raft is a distributed system algorithm that uses server replication to update state machine logs. It has been used in many big name distributed systems, and has applications in synchronization, barrier orchestration, and configuration management.

Zookeeper uses consensus mechanisms to replicate the configuration, like leader election group membership service Discovery management of metadata. All of that is being done by a consensus mechanism implemented in it and basically the distributed message cues that you cue one message and deliver it to everyone. Um, that is written like basically which could be like to one uh receiver or multiple receivers because it's based on the concept of atomic broadcast which gives you two promises give only one copy of the message to everyone and uh like basically make sure that you provide a copy to everyone that message that's um is a special case of the consensus algorithm as well which is basis of doing a lot of implementation of Pop Q messages and pop subsystems. So it is consensus algorithm is a very important uh algorithm when we want to build distributed system because of the dependence of this many sets of problem and distributed systems that we have to it. And if we want to name some of the uh like in the last few seconds I'm sorry it's two minutes remaining but this is uh small this is some of the famous names that you will see um like basically implemented with some version of uh paxos and rafter in them so pixels that have Chubbies to be care in TCP uh bookkeeper all of them are um using some part of this uh algorithm in them um we are not going to discuss these but if you're interested there is a basically paper 2016 paper which is paxos in the cloud it discusses like how why it hacks those is as important as it is and basically discusses different kind of use cases that like we just said like what type of problem it is going to be applied on it gives you pointers to where it is implemented and where you can find the details of each implementation on that with that in the last two minutes I'm going to wrap up the discussion that we had on non-presenting type uh like fault tolerance and related mechanisms as we discussed which is consensus in the next session we are going to discuss uh like basically um what we are going to do for presenting type of thoughts so by far we have discussed um for uh are distributed systems availability performance scalability consistency and fault tolerance in different kind of you know parameters that we need from a distributed system and our discussion in the last few modules were specifically on fault tolerance we said that how we are going to follow it all um false colors faults in our system is by replicating uh the state of like basically when we have distributed system we have uh replicated systems working on them and we want to keep those systems working in the same state we want to keep this consistent state of our system we said that if we do not have malicious system uh like players in our system we can tolerate a set of faults by coming to conclusion what majority of those nodes are saying the condition that we are discussing in the Byzantine false case is that if we have nodes that can Collide and be non-uh

The talk is about how non-honest players can take over a system, and how to build systems that can resist this. The most famous example is Bitcoin, which is based on anyone being able to join. The challenge is to think about how to keep a distributed system working when there are players who can attack it.