The first quiz of the course will be on Thursday and will be available on canvas. The quiz will be open book, but no internet access will be allowed. The quiz will be worth 1/4 of the total grade for the course. The first homework assignment is due tonight, and the second homework assignment will become available tonight as well. The project topics are due next week.

This section will cover some of the details of store resources in a distributed system. It will discuss how store resources are used to meet the needs of fast transactions and how they can be scaled to improve performance.

Systems use different types of memory, including flash memory and SSC, for different purposes. Network storage is a type of secondary storage, which is slower than primary storage but can be used for archiving data. Shared memory, shared disk, and shared nothing are the three main types of systems for data.

1. Many systems that we use are actually shared nothing systems, where each system has its own separate memory and processing. 2. A paper called "The Case for Shared Nothing Architecture" by Michael Stonebreaker compares the cost and efficiency of different storage options. 3. Shared nothing architecture is often used in cloud and distributed systems because it is easy to build with commodity hardware. 4. Shared memory architectures are used when we need fast read and write access to data, while shared disk architectures are used when we have huge volumes of data to store.

There are three main types of cloud storage: in-memory, block, and file. In-memory storage is used for applications that require real-time interactions, while block storage is used for databases and analytics. File storage is the next level up, and is used for storing large files.

Different types of storage are better suited for different purposes, and so it is important to choose the right type of storage when designing a system. File storage is a type of storage that is well suited for content repositories and media stores, while object storage is better suited for data warehousing and data lakes.

The technology used for cashless transactions can affect the speed of those transactions. The software written for accessing those technologies can also affect how fast or slow the transactions are. Additionally, the way an application manages itself can also affect transaction speed.

The article discusses different types of consistency models for distributed systems, specifically data-centric consistency models. It defines what it means for data to be consistent in these models and outlines some of the specific models that fall under this category.

There are four main types of consistency models for distributed systems: strict, linearizable, sequential, and causal. Strict consistency is the most difficult to achieve, as it requires all processes to see shared accesses in the same global order and at the exact same time. Linearizable consistency is a bit more relaxed, allowing for any order that still makes sense globally. Sequential consistency only requires that all processes see shared accesses in the same order, without specifying a time frame. Causal consistency only requires that causally related processes see shared accesses in the same order.

Processes 1 and 2 have a causal relationship, which means that one of them is writing a parameter X that is read by the other one. If they are both builders of the system, they need to guarantee that the system is built in a way that maintains consistency. However, if they are both users of the system, they may not have control over how the system is built, and so they need to be aware of the consistency guarantees that the system provides.

The writer processes (P1 and P2) write values to a resource (X), and the reader process (P3) reads from that resource. The strict consistency model would not work in this case, because the processes are not happening at the same time. The linearizable consistency model would not work either, because there is no global order of events that would cause the reads and writes to be consistent. The sequential consistency model does work, because the processes are still sequential and the information is propagated between them in a way that causes the final result to be consistent.

The text describes how global ordering of events does not always lead to consistent results, and how the causal consistency model accounts for this. Causal consistency says that all processes that share a resource see causally related shared accesses in the same order. This means that if two processes are reading and writing to the same resource, they will see the events in the same order. This is different from the global ordering model, which would say that all processes see all events in the same order, regardless of whether or not they share a resource.

The definition of a consistent data consistency model comes from the definition of a distributed shared memory. This means that the effect of what is happening locally should be available globally, but it may take some time for this information to propagate throughout the system.

We might be in an inconsistent state in one way or another, that's what answer your question or something else because you have to synchronize everything at the end right? If you're synchronized, if you block it so I write something and I have different instances in different places that I have to update right? For this process this process this process and we said that each one of them have their own memory but we make it we make them think they are sharing the same memory. So from that time that it takes for the update it happens in one process till the update takes effect in the other process there is a gap if we don't lock if we lock that so that they are taking effects we are like basically locking until the new value is available right? If we are not doing that there is the time that they they might be different so we are taking risk in that time that is taking for them to be available and during that time we might or might not see these consistency patterns consistent. We have different scenarios in wataka the one rights happened before right? It's actually one of the people right having the 41 and so if you have a cynical shared memory I mean we are saying causal right? So and we are saying these are separate processes so while we are not locking them having that kind of thing so we are not consistent in that sense so that's uh like Central um shared memory is not consistent in that sense of global consistency but the ones that have carbon dependencies still can work. What what effect does it have? This kind of thinking means that if we have implemented a program that like needs updates to be done on these like local updates some parts of it is still acceptable for US based on which one affects the other one which one of the processes is going to affect the data of our other one if those are propagated is in the system we are still fine to run this system if we that is our requirement means that for example I I do have different clusters of my work happening or for example different geographical reasons uh regions and while the requests that are coming that are um like need updates in these places are still maintaining the data in a correct State I am fine right while they might give uh you know they might not be completely uh in a Global Order of things happening doesn't answer your question okay? I think if if we go a little bit further you see some examples and you see like how in real world it is used uh it will clear it a little bit more but I didn't bring because I thought that this is something many of you have seen in distributed systems before yes that is okay maybe another thing I just brought I I just wanted to review this table with you each one of these have formal definition of meeting risk requirements so you can see like I meet these requirements I meet this requirement that is going to be easier to say but the sequential basically going just from this definition all processes see all shared accesses in a same order so can you have a process see something before another process sees it no because if it's before it means it's not sequential so these definitions here they help us understand which ones we should use for our programs

There is a global order of events that all processes must follow in order to maintain consistency. This order is determined by the writes and reads that each process does. For example, if process 1 writes data to a shared location, process 2 must read that data before process 3 can write to the same location. This ensures that all processes see the same data in the same order.

There are different types of data consistency, one of which is when data is only consistent after synchronization operations have been completed. Another type is weak consistency, where data is only considered consistent after a short period of time. There is also release consistency, where shared data is made consistent when a critical region is exited, and entry consistency, where shared data is made consistent when a critical region is entered.

The article discusses the concept of consistency in distributed systems, and how it can be defined in different ways depending on the needs of the system. It also introduces the concept of eventual consistency, which is when updates eventually reach a consistent state after a period of time.

The Cap Theorem or Bruce Conjecture states that it is impossible to have all three of the requirements (consistency, availability, and partition tolerance) in a distributed system. However, you can have two of them at a time. This means that you can have availability and consistency, consistency and partition tolerance, or availability and partition tolerance.

The CAP theorem is a paper published in 2002 that argues that availability is affected in a distributed system when implementing consistency. The pastel theorem is a response to the CAP theorem that takes into account the linearizability of the system. In an Active Learning example, a system is designed to meet the requirements of consistency and availability, but the eventual consistency of the system means that the system is not completely available.